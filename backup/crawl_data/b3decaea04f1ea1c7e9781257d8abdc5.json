{"url": "https://grape.ics.uci.edu/wiki/public/wiki/cs221-2019-spring-project1?action=diff&version=16", "word_count": 1531, "words": ["cs221-2019-spring-project1", "(diff)", "\u2013", "Public", "Login", "Preferences", "About", "Trac", "Register", "Context", "Navigation", "\u2190", "Previous", "Change", "Wiki", "History", "Next", "Change", "\u2192", "Changes", "between", "Version", "15", "and", "Version", "16", "of", "cs221-2019-spring-project1", "View", "differences", "inline", "side", "by", "side", "Show", "lines", "around", "each", "change", "Show", "the", "changes", "in", "full", "context", "Ignore:", "Blank", "lines", "Case", "changes", "White", "space", "changes", "Timestamp:", "Apr", "2,", "2019,", "1:49:35", "PM", "(", "7", "years", "ago)", "Author:", "chenli", "Comment:", "--", "Legend:", "Unmodified", "Added", "Removed", "Modified", "cs221-2019-spring-project1", "v15", "v16", "13", "13", "14", "14", "==", "Coding", "Tasks", "==", "15", "1.", "Implement", "a", "simple", "tokenizer", "based", "on", "punctuations", "and", "white", "space", ".", "(3", "points)", "15", "1.", "Implement", "a", "simple", "tokenizer", "based", "on", "punctuations", "and", "white", "space", "s", ".", "(3", "points)", "16", "16", "1.", "Implement", "a", "Dynamic-Programming-based", "Word-Break", "Tokenizer.", "(7", "points)", "17", "17", "1.", "Incorporate", "a", "Porter", "stemmer.", "(2", "points)", "\u2026", "\u2026", "49", "49", "=", "Coding", "Tasks", "=", "50", "50", "51", "==", "Task", "1:", "Implement", "a", "simple", "punctuations", "and", "white", "spaces", "based", "tokenizer", "(3", "points)", "==", "51", "==", "Task", "1:", "Implement", "a", "simple", "tokenizer", "based", "on", "punctuations", "and", "white", "spaces", "(3", "points)", "==", "52", "52", "53", "53", "Implement", "this", "tokenizer", "in", "`analysis/PunctuationTokenizer.java`", "\u2026", "\u2026", "56", "56", "57", "57", "Requirements:", "58", "-", "White", "spaces", "(space,", "tab,", "newline,", "etc..)", "and", "punctuations", "provided", "b", "elow", "should", "be", "used", "to", "tokenize", "the", "text.", "59", "-", "White", "spaces", "and", "punctuations", "should", "be", "removed", "from", "the", "token", "and", "result", "s.", "58", "-", "White", "spaces", "(space,", "tab,", "newline,", "etc..)", "and", "punctuations", "provided", "b", "y", "us", "should", "be", "used", "to", "tokenize", "the", "text.", "59", "-", "White", "spaces", "and", "punctuations", "should", "be", "removed", "from", "the", "result", "token", "s.", "60", "60", "-", "All", "tokens", "should", "be", "converted", "to", "lower", "case.", "61", "-", "Stop", "words", "should", "be", "filtered", "out.", "Use", "the", "stop", "word", "list", "provided", "in", "`StopWords.java`", "61", "-", "Stop", "words", "should", "be", "filtered", "out.", "Use", "the", "stop", "word", "list", "provided", "in", "`StopWords.java`", ".", "62", "62", "63", "63", "64", "==", "Task", "2:", "Implement", "a", "Dynamic-Programming", "based", "Word", "Break", "Tokenizer", "(7", "points)", "==", "64", "==", "Task", "2:", "Implement", "a", "Dynamic-Programming", "based", "Word", "-", "Break", "Tokenizer", "(7", "points)", "==", "65", "65", "66", "Word", "break", "is", "a", "problem", "where", "given", "a", "dictionary", "and", "a", "string", "(text", "with", "all", "white", "spaces", "removed),", "determine", "how", "to", "break", "the", "string", "into", "sequence", "of", "words.", "67", "Implement", "this", "tokenizer", "in", "`analysis/PunctuationTokenizer.java`", "As", "an", "example,", "input", "string", "`\"catanddog\"`", "should", "be", "broken", "to", "tokens", "`[\"cat\",", "\"and\",", "\"dog\"]`", "66", "Word", "break", "is", "a", "problem", "where", "given", "a", "dictionary", "and", "a", "string", "(text", "with", "all", "white", "spaces", "removed),", "determine", "how", "to", "break", "the", "string", "into", "a", "sequence", "of", "words.", "67", "Implement", "this", "tokenizer", "in", "`analysis/PunctuationTokenizer.java`", ".", "As", "an", "example,", "an", "input", "string", "`\"catdog\"`", "should", "be", "broken", "to", "tokens", "`[\"cat\",", "\"dog\"]`.", "68", "68", "69", "Use", "frequency", "statistics", "to", "choose", "the", "optimal", "way", "when", "there", "are", "many", "alternatives", "to", "break", "a", "string.", "For", "example", "69", "Use", "frequency", "statistics", "to", "choose", "the", "optimal", "way", "when", "there", "are", "many", "alternatives", "to", "break", "a", "string.", "For", "example", ":", "70", "70", "{{{", "71", "71", "input", "string", "is", "\"ai\",", "\u2026", "\u2026", "80", "80", "81", "81", "Requirements:", "82", "-", "Use", "Dynamic", "Programming", "for", "efficiency", ".", "Running", "time", "cannot", "be", "too", "slow", ".", "83", "-", "Use", "the", "the", "given", "dictionary", "corpus", "and", "frequency", "statistics", "to", "determine", "optimal", "alternative.", "82", "-", "Use", "Dynamic", "Programming", "for", "efficiency", "purposes", ".", "83", "-", "Use", "the", "given", "dictionary", "corpus", "and", "frequency", "statistics", "to", "determine", "an", "optimal", "alternative.", "84", "84", "The", "probability", "is", "calculated", "as", "the", "product", "of", "each", "token's", "probability,", "assuming", "the", "tokens", "are", "independent.", "85", "-", "A", "match", "in", "dictionary", "is", "case", "insensitive.", "Output", "tokens", "should", "all", "be", "in", "lower", "case.", "86", "-", "Stop", "words", "should", "be", "removed", "after", "breaking", ".", "85", "-", "A", "match", "in", "the", "dictionary", "is", "case", "insensitive.", "Output", "tokens", "should", "all", "be", "in", "lower", "case.", "86", "-", "Stop", "words", "should", "be", "removed", ".", "87", "87", "-", "If", "there's", "no", "possible", "way", "to", "break", "the", "string,", "throw", "an", "exception.", "88", "88", "89", "89", "90", "==", "Task", "3:", "Incorporate", "a", "p", "orter", "stemmer", "(2", "points)", "==", "90", "==", "Task", "3:", "Incorporate", "a", "P", "orter", "stemmer", "(2", "points)", "==", "91", "91", "92", "92", "Stemming", "is", "the", "process", "of", "reducing", "a", "word", "into", "its", "\"stem\"", "(\"root\")", "form.", "93", "93", "94", "94", "Porter", "stemming", "is", "a", "classic", "and", "popular", "algorithm", "that", "uses", "a", "set", "of", "rules", "and", "steps", "to", "process", "a", "token.", "95", "Implementing", "porter", "stemmer", "is", "not", "technically", "interesting", "-", "it", "just", "consists", "a", "bunch", "of", "if-else", "statements,", "96", "therefore", "we", "ask", "you", "to", "simply", "incorporate", "an", "existing", "porter", "stemmer", "implementation", "into", "this", "project.", "95", "We", "ask", "you", "to", "incorporate", "the", "following", "existing", "Porter", "stemmer", "implementation", "into", "this", "project:", "97", "96", "98", "97", "https://github.com/apache/lucene-solr/blob/master/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/PorterStemmer.java", "\u2026", "\u2026", "101", "100", "==", "Task", "4:", "Implement", "a", "dynamic-programming", "based", "Chinese", "or", "Japanese", "tokenizer", "(Optional", "Extra", "Credit,", "3", "points)", "==", "102", "101", "103", "Tokenizing", "Chinese", "or", "Japanese", "text", "is", "challenging", "because", "there", "'s", "no", "spaces", "between", "words.", "It", "is", "very", "similar", "to", "the", "word", "break", "problem", "in", "task", "2.", "102", "Tokenizing", "Chinese", "or", "Japanese", "text", "is", "challenging", "because", "there", "are", "no", "explicit", "spaces", "between", "words.", "It", "is", "very", "similar", "to", "the", "word-", "break", "problem", "in", "task", "2.", "104", "103", "105", "Use", "the", "same", "dictionary-frequency", "and", "dynamic", "programming", "based", "algorithm", "in", "task", "2", "to", "implement", "a", "Chinese", "or", "Japanese", "Tokenizer.", "For", "fairness,", "you", "must", "choose", "a", "language", "that", "is", "NOT", "your", "mother", "language.", "104", "Use", "the", "same", "dictionary-frequency", "and", "dynamic", "programming", "based", "algorithm", "in", "task", "2", "to", "implement", "a", "Chinese", "or", "Japanese", "Tokenizer.", "For", "fairness,", "you", "must", "choose", "a", "language", "that", "is", "NOT", "your", "native", "language.", "106", "105", "107", "106", "You", "need", "to", "find", "a", "Chinese", "or", "Japanese", "dictionary", "corpus", "with", "frequency", "information", "on", "your", "own,", "and", "write", "at", "least", "3", "test", "cases", "to", "test", "the", "correctness", "of", "your", "tokenizer.", "\u2026", "\u2026", "111", "110", "==", "Task:", "Submitting", "Test", "Cases", "(3", "points)", "==", "112", "111", "113", "For", "this", "project,", "we", "require", "each", "team", "to", "submit", "at", "least", "2", "test", "cases.", "We", "expect", "you", "to", "write", "high-quality", "test", "cases,", "and", "grade", "will", "be", "based", "on", "the", "correctness,", "quality,", "and", "documentation", "of", "them", ".", "112", "For", "this", "project,", "we", "require", "each"], "content_hash": "371ce680e635aabbfce9e9e0a77b124c"}